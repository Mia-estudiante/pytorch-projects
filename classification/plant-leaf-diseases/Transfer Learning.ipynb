{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMf2mw6FD1MI40aYR09baOF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Step1. Ready for Tranfer Learning"],"metadata":{"id":"WXoeeYG6C82R"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader"],"metadata":{"id":"ZImUE0edV471"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize([64,64]),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomCrop(52), #이미지 일부를 랜덤하게 잘라내어 52*52 사이즈로 변경\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], #입력 데이터 정규화(평균, 표준편차)\n","                             [0.229, 0.224, 0.225]) #데이터 정규화는 모델을 최적화하며 Local Minimum에 빠지는 것 방지\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize([64,64]),\n","        transforms.RandomCrop(52),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"metadata":{"id":"F_xHszxnVuar","executionInfo":{"status":"ok","timestamp":1684203323982,"user_tz":-540,"elapsed":14,"user":{"displayName":"Eunae Kang","userId":"01300216816198274349"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["data_dir = './splitted'\n","\n","#datasets 준비 + transform\n","image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x])\n","                  for x in ['train', 'val']}\n","                  \n","#미니 배치로 분리하는 역할\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n","                  batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","class_names = image_datasets['train'].classes"],"metadata":{"id":"jH3rwhxSVxbt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step2. Load Pre-Trained Model"],"metadata":{"id":"B2zQ-ctLeyuF"}},{"cell_type":"code","source":["from torchvision import models"],"metadata":{"id":"pwdB9VPGeyLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. Pre-Trained 모델 불러오기\n","resnet = models.resnet50(pretrained=True) \n","#pretrained=True: 미리 학습된 모델의 파라미터 값 그대로 가져옴\n","#          =False: 모델의 구조만 가져오고 파라미터 값 랜덤으로 설정\n","\n","num_ftrs = resnet.fc.in_features  \n","resnet.fc = nn.Linear(num_ftrs, 33) #우리 주제에 맞는 채널의 수를 출력하는 layer 추가 \n","resnet = resnet.to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters(), lr=0.001))\n","'''\n","일부 layer의 파라미터만을 업데이트하기 위해\n","requires_grad = True 로 설정된 layer의 파라미터에만 적용\n","'''"],"metadata":{"id":"teHzQc5Ze6My"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import lr_scheduler\n","#lr_scheduler.StepLR()\n","#Epoch 따라 lr을 변경하는 역할, 즉 gamma만큼 곱해 lr을 감소"],"metadata":{"id":"03f--Ao1hiiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"metadata":{"id":"jH61_l-thoHh"},"execution_count":null,"outputs":[]}]}