{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMYmhUTObVQ/HhklE8gXUY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Step1. Ready for Tranfer Learning"],"metadata":{"id":"WXoeeYG6C82R"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader"],"metadata":{"id":"ZImUE0edV471"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize([64,64]),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomCrop(52), #이미지 일부를 랜덤하게 잘라내어 52*52 사이즈로 변경\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], #입력 데이터 정규화(평균, 표준편차)\n","                             [0.229, 0.224, 0.225]) #데이터 정규화는 모델을 최적화하며 Local Minimum에 빠지는 것 방지\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize([64,64]),\n","        transforms.RandomCrop(52),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"metadata":{"id":"F_xHszxnVuar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = './splitted'\n","\n","#datasets 준비 + transform\n","image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x])\n","                  for x in ['train', 'val']}\n","                  \n","#미니 배치로 분리하는 역할\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n","                  batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","class_names = image_datasets['train'].classes"],"metadata":{"id":"jH3rwhxSVxbt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step2. Load Pre-Trained Model"],"metadata":{"id":"B2zQ-ctLeyuF"}},{"cell_type":"code","source":["from torchvision import models"],"metadata":{"id":"pwdB9VPGeyLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. Pre-Trained 모델 불러오기\n","resnet = models.resnet50(pretrained=True) \n","#pretrained=True: 미리 학습된 모델의 파라미터 값 그대로 가져옴\n","#          =False: 모델의 구조만 가져오고 파라미터 값 랜덤으로 설정\n","\n","num_ftrs = resnet.fc.in_features  \n","resnet.fc = nn.Linear(num_ftrs, 33) #우리 주제에 맞는 채널의 수를 출력하는 layer 추가 \n","resnet = resnet.to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n","'''\n","일부 layer의 파라미터만을 업데이트하기 위해\n","requires_grad = True 로 설정된 layer의 파라미터에만 적용\n","'''"],"metadata":{"id":"teHzQc5Ze6My"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import lr_scheduler\n","#lr_scheduler.StepLR()\n","#Epoch 따라 lr을 변경하는 역할, 즉 gamma만큼 곱해 lr을 감소"],"metadata":{"id":"03f--Ao1hiiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"metadata":{"id":"jH61_l-thoHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step3. Freeze some layers of Pre-Trained Model"],"metadata":{"id":"pGp1V0FQifFk"}},{"cell_type":"code","source":["'''\n","상위 1-5번의 layer는 파라미터 업데이트하지 않도록 고정\n","하위 6-10번의 layer는 파라미터 학습 과정에서 업데이트하도록\n","'''\n","ct = 0\n","for child in resnet.children(): #resnet.children()은 resnet 모델의 모든 층 정보를 가지고 있음\n","  ct += 1\n","  if ct < 6:\n","    for param in child.parameters():\n","      param.requires_grad = False"],"metadata":{"id":"ZCZeJc3KioHq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step4. Make the function to train and validate model"],"metadata":{"id":"KfmimIeGKDTf"}},{"cell_type":"code","source":["def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  best_acc = 0.0\n","\n","  for epoch in range(num_epochs):\n","    print('----------------epoch {}-------------------'.format(epoch+1))\n","    since = time.time()\n","\n","    for phase in ['train', 'test']:\n","      if phase == 'train':\n","        model.train() #학습 모드 설정\n","      else:\n","        model.eval() #평가 모드 설정\n","      \n","      running_loss = 0.0\n","      running_corrects = 0\n","\n","      for inputs, labels in dataloaders[phase]:\n","        inputs = inputs.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        optimizer.zero_grad() #이전 batch의 gradient가 optimizer에 저장되어 있으니 이를 초기화\n","\n","        with torch.set_grad_enabled(phase=='train'):\n","          #torch.set_grad_enabled() 는 모델의 gradient를 업데이트하고 하지 않기 위해 사용\n","          #학습 단계에서만 모델 gradient 업데이트, 검증 단계에서는 업데이트 x\n","          outputs = model(inputs)\n","          _, preds = torch.max(outputs, 1) #가장 높은 값을 가진 것을 예측값으로 저장\n","          loss = criterion(outputs, labels)\n","\n","          if phase == 'train':\n","            loss.backward()  #위 loss를 통해 역전파를 수행하고 계산된 gradient 값을 각 파라미터에 할당\n","            optimizer.step() #모델의 파라미터 업데이트\n","\n","        running_loss += loss.item()*inputs.size(0) #모든 데이터 총 loss\n","\n","          '''\n","          모든 데이터의 loss를 합산해서 저장하기 위해 하나의 배치에 대해 계산된 loss값에 데이터 수 곱함\n","          inputs.size(0): DataLoader에서 전달되는 미니 배치의 데이터 수로 배치 사이즈\n","\n","          '''\n","        running_corrects += torch.sum(preds==labels.data) #모든 데이터의 예측값 맞았는지 \n","        \n","      if phase=='train':\n","        scheduler.step()\n","\n","\n","          l_r = [x['lr'] for x in optimizer_ft.param_groups]\n","          print('learning rate: ', l_r)\n","        '''\n","        optimizer_ft.param_groups의 원소는 학습 과정에서의 파라미터를 저장하는 딕셔너리\n","        스케줄러에 의해 lr 조정 확인\n","        '''\n","\n","        epoch_loss = running_loss/dataset_sizes[phase]\n","        epoch_acc = running_corrects.double()/dataset_sizes[phase]\n","\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_oss, epoch_acc))\n","\n","        if phase=='val' and epoch_acc > best_acc:\n","          best_acc = epoch_acc\n","          best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    time_elapsed = time.time() - since\n","    print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n","  print('Best val Acc: {:4f}'.format(best_acc))\n","\n","  model.load_state_dict(best_model_wts)\n","\n","  return model"],"metadata":{"id":"kMyCDJN7i0-o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step5. Execute model training"],"metadata":{"id":"O8lcslhjWTKI"}},{"cell_type":"code","source":["model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, \n","                              exp_lr_scheduler, num_epochs=EPOCH)\n","torch.save(model_resnet50, 'resnet50.pt')"],"metadata":{"id":"cWeqNmWqUJvU"},"execution_count":null,"outputs":[]}]}