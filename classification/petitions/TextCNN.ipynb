{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdYkLbNmPH60bwJYd0ansd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###Step1. Make the TextCNN Model"],"metadata":{"id":"vDA6Y1mihGVz"}},{"cell_type":"code","source":["import torch.nn as nn #신경망 구축을 위한 데이터 구조나 layer 등의 라이브러리\n","import torch.optim as optim #파라미터 최적화 알고리즘 제공\n","import torch.nn.functional as F #활성화 함수, 손실 함수 등 신경망 제작에 쓰이는 함수 제공"],"metadata":{"id":"--yIRKGihHil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextCNN(nn.Module):\n","  \n","  '''\n","  vocab_built: Train 데이터로 생성한 단어장을 인자로 받음\n","  emb_dim: 임베딩 벡터의 크기\n","  dim_channel: feature map 이후 생성되는 채널의 수\n","  kernel_wins: filter의 크기를 리스트 형태로 받음\n","  num_class: output 클래스 개수\n","  '''\n","  def __init__(self, vocal_built, emb_dim, dim_channel, kernel_wins, num_class):\n","\n","    super(TextCNN, self).__init__()\n","\n","    self.embed = nn.Embedding(len(vocab_built), emb_dim) #임베딩 설정,\n","                                                           #vocab_size * embedding dimension\n","    self.embed.weight.data.copy_(vocab_built.vectors)      #Word2Vec로 학습한 embedding vector 값\n","    \n","    self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n","    \n","    self.relu = nn.ReLU() #activation map 생성\n","    self.dropout = nn.Dropout(0.4) #overfitting 방지, 즉 가중치 행렬에서 랜덤 40%에 0을 넣어 연산\n","    self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_classes)    #score vector 생성\n","    \n","    \n","  def forward(self, x):\n","    emb_x = self.embed(x)\n","    emb_x = emb_x.unsqueeze(1) #batch*1*vocab size*embedding vector\n","                               #2차원의 텍스트 데이터를 모델에 이미지처럼 입력하기 위해 3차원 형태로 변환해야 함\n","    \n","    con_x = [self.relu(conv(emb_x)) for conv in self.convs]\n","    \n","    pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in convs]\n","    \n","    fc_x = torch.cat(pool_x, dim=1)\n","    fc_x = fc_x.squeeze(-1)\n","    fc_x = self.dropout(fc_x)\n","    \n","    logit = self.fc(fc_x)\n","    \n","    return logit       "],"metadata":{"id":"GUYOhr7NwJaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pe4BhnOvt1Q5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step2. Make the train function"],"metadata":{"id":"CNFymwaft65c"}},{"cell_type":"code","source":["def train(model, device, train_itr, optimizer):\n","\n","  model.train()\n","  corrects, train_loss = 0.0, 0\n","\n","  for batch in train_iter: #batch 사이즈별로 train\n","\t\t\t\t\t\t\t\t\t\t\t\t\t#batch가 곧 dataset\n","    text, target = batch.text, batch.label #TEXT, LABEL; \n","\n","\n","    text = torch.transpose(text, 0, 1) #전치행렬 적용\n","    target.data.sub_(1)\n","    text, target = text.to(device), target.to(device)\n","\n","    ######################################\n","\n","\t\toptimizer.zero_grad() #optimizer의 gradient 초기화\n","    logit = model(text) #예측값\n","    loss = F.cross_entropy(logit, target) #loss 값\n","    loss.backward() #back propogation\n","    optimizer.step() #파라미터 별 할당된 gradient 값 이용해 파라미터 업데이트\n","\n","    train_loss += loss.item() #미니 배치 loss 누적\n","    result = torch.max(logit, 1)[1] #행 기준으로 제일 큰 값의 인덱스 출력\n","    corrects += (result.view(target.size()).data == target.data).sum()\n","\n","  train_loss /= len(train_iter.dataset)\n","  accuacy = 100.0*corrects / len(train_itr.dataset)\n","\n","  return train_loss, accuracy"],"metadata":{"id":"zhlqtKtEt1Vt"},"execution_count":null,"outputs":[]}]}